<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-11-12T19:53:51+03:00</updated><id>/feed.xml</id><title type="html">TomDev24 Blog</title><subtitle>sic parvis magna</subtitle><author><name>TomDev</name><email>tommog2499@gmail.com</email></author><entry xml:lang="ru"><title type="html">Python async and threads</title><link href="/programming/2023/10/27/python_async_and_threads.html" rel="alternate" type="text/html" title="Python async and threads" /><published>2023-10-27T00:00:00+03:00</published><updated>2023-10-27T00:00:00+03:00</updated><id>/programming/2023/10/27/python_async_and_threads</id><content type="html" xml:base="/programming/2023/10/27/python_async_and_threads.html"><![CDATA[<p>In computer programming, a runtime library is a set of low-level routines used by a compiler to invoke some of the behaviors of a runtime environment, by inserting calls to the runtime library into compiled executable binary. The runtime environment implements the execution model, built-in functions, and other fundamental behaviors of a programming language.[1] During execution (run time) of that computer program, execution of those calls to the runtime library cause communication between the executable binary and the runtime environment. A runtime library often includes built-in functions for memory management or exception handling.[2] Therefore, a runtime library is always specific to the platform and compiler.</p>

<p>The runtime library may implement a portion of the runtime environment’s behavior, but if one reads the code of the calls available, they are typically only thin wrappers that simply package information, and send it to the runtime environment or operating system. However, sometimes the term runtime library is meant to include the code of the runtime environment itself, even though much of that code cannot be directly reached via a library call.</p>

<p>The concept of a runtime library should not be confused with an ordinary program library like that created by an application programmer or delivered by a third party, nor with a dynamic library, meaning a program library linked at run time. For example, the C programming language requires only a minimal runtime library (commonly called crt0), but defines a large standard library (called C standard library) that has to be provided by each implementation.[1]</p>

<p>Runtime describes software/instructions that are executed while your program is running, especially those instructions that you did not write explicitly, but are necessary for the proper execution of your code.</p>

<p>Low-level languages like C have very small (if any) runtime. More complex languages like Objective-C, which allows for dynamic message passing, have a much more extensive runtime.</p>

<p>You are correct that runtime code is library code, but library code is a more general term, describing the code produced by any library. Runtime code is specifically the code required to implement the features of the language itself.</p>

<p>Go does have an extensive library, called the runtime, that is part of every Go program. The runtime library implements garbage collection, concurrency, stack management, and other critical features of the Go language. Although it is more central to the language, Go’s runtime is analogous to libc, the C library.</p>

<p>It is important to understand, however, that Go’s runtime does not include a virtual machine, such as is provided by the Java runtime. Go programs are compiled ahead of time to native machine code (or JavaScript or WebAssembly, for some variant implementations). Thus, although the term is often used to describe the virtual environment in which a program runs, in Go the word “runtime” is just the name given to the library providing critical language services.</p>

<p>There are a lot of differences, and for the answer to be complete, you would need to specify which language you wanted to compare it to. But on a really simple level, thwd’s answer is more or less correct. A VM language is usually compiled into an instruction set for that VM. The VM then provides a lot of “special sauce.” Go is (usually) compiled directly into machine code to be executed directly on the target system.</p>

<p>One consequence of this is that the executable can be run without having any other software installed on the machine. It also  means that the code for the stuff you inquired about such as the garbage collector, goroutine scheduling and stack management, is all present in the single executable compiled by go.</p>

<p>IMHO although the runtime and VM both provide facilities such as garbage collection, scheduling ect they are not alike at all. Actually that is the ONLY way they are alike. VM run compiled bytcode like others stated, but the VM is a whole program on its own, which is run, and handled by the os like a compiled-to-machine code program would be, and then that (VM) program reads and runs the bytecode compiled program.</p>

<p>Whereas instead of being a seperate program in it’s own right, Go’s runtime is actually just a set of libraries that are compiled into every Go binary. Just like the std libraries that you import into your program (ie “fmt”) they are compiled and linked along with user code into a single program. In fact, you can also import these libraries into your code manually and play with it; with a VM you don’t have the same ability. (*some VMs can be ‘tuned’, but only from knobs that are specifically givin to you, not arbitrarily)</p>

<p>The end result is that you get all the facilities for managing memory, goroutine schedualing, ect that you want from a virtual machine, but interwoven into your code, which as a whole is then compiled down to raw machine code.</p>

<p>If you want to learn more about how this is all implemented, I find that the go source is extremely approchable, and even a beginner should be able to get some idea about what the code is doing. (“$GOROOT/src/runtime is a good starting place)</p>

<p>In computer programming, a green thread (virtual thread) is a thread that is scheduled by a runtime library or virtual machine (VM) instead of natively by the underlying operating system (OS). Green threads emulate multithreaded environments without relying on any native OS abilities, and they are managed in user space instead of kernel space, enabling them to work in environments that do not have native thread support.[1]</p>

<h3 id="python">Python</h3>

<p>Python internals:
https://leanpub.com/insidethepythonvirtualmachine/read
https://www.youtube.com/watch?v=XGF3Qu4dUqk
https://github.com/python/cpython/tree/main</p>

<h3 id="stackless-python">Stackless Python</h3>

<p>Stackless Python, or Stackless, is a Python programming language interpreter, so named because it avoids depending on the C call stack for its own stack. In practice, Stackless Python uses the C stack, but the stack is cleared between function calls.[2] The most prominent feature of Stackless is microthreads, which avoid much of the overhead associated with usual operating system threads. In addition to Python features, Stackless also adds support for coroutines, communication channels, and task serialization.</p>

<p>With Stackless Python, a running program is split into microthreads that are managed by the language interpreter itself, not the operating system kernel—context switching and task scheduling is done purely in the interpreter (these are thus also regarded as a form of green thread). Microthreads manage the execution of different subtasks in a program on the same CPU core. Thus, they are an alternative to event-based asynchronous programming and also avoid the overhead of using separate threads for single-core programs (because no mode switching between user mode and kernel mode needs to be done, so CPU usage can be reduced).</p>

<p>Although microthreads make it easier to deal with running subtasks on a single core, Stackless Python does not remove CPython’s Global Interpreter Lock, nor does it use multiple threads and/or processes. So it allows only cooperative multitasking on a shared CPU and not parallelism (preemption was originally not available but is now in some form[3]). To use multiple CPU cores, one would still need to build an interprocess communication system on top of Stackless Python processes.</p>

<p>Due to the considerable number of changes in the source, Stackless Python cannot be installed on a preexisting Python installation as an extension or library. It is instead a complete Python distribution in itself. The majority of Stackless’s features have also been implemented in PyPy, a self-hosting Python interpreter and JIT compiler.[4]</p>

<p>Stackless Python’s main benefit is the support for very lightweight coroutines. CPython doesn’t support coroutines natively (although I expect someone to post a generator-based hack in the comments) so Stackless is a clear improvement on CPython when you have a problem that benefits from coroutines.</p>

<p>I think the main area where they excel are when you have many concurrent tasks running within your program. Examples might be game entities that run a looping script for their AI, or a web server that is servicing many clients with pages that are slow to create.</p>

<p>You still have many of the typical problems with concurrency correctness however regarding shared data, but the deterministic task switching makes it easier to write safe code since you know exactly where control will be transferred and therefore know the exact points at which the shared state must be up to date.</p>

<h3 id="python-multithreading">Python multithreading</h3>

<p>https://tenthousandmeters.com/blog/python-behind-the-scenes-13-the-gil-and-its-effects-on-python-multithreading/
https://stackoverflow.com/questions/52507601/whats-the-point-of-multithreading-in-python-if-the-gil-exists</p>

<h2 id="python-async">Python Async</h2>

<p>https://realpython.com/async-io-python/</p>

<p>You may be thinking with dread, “Concurrency, parallelism, threading, multiprocessing. That’s a lot to grasp already. Where does async IO fit in?”</p>

<p>Parallelism consists of performing multiple operations at the same time. Multiprocessing is a means to effect parallelism, and it entails spreading tasks over a computer’s central processing units (CPUs, or cores). Multiprocessing is well-suited for CPU-bound tasks: tightly bound for loops and mathematical computations usually fall into this category.</p>

<p>Concurrency is a slightly broader term than parallelism. It suggests that multiple tasks have the ability to run in an overlapping manner. (There’s a saying that concurrency does not imply parallelism.)</p>

<p>Threading is a concurrent execution model whereby multiple threads take turns executing tasks. One process can contain multiple threads. Python has a complicated relationship with threading thanks to its GIL, but that’s beyond the scope of this article.</p>

<p>In fact, async IO is a single-threaded, single-process design: it uses cooperative multitasking, a term that you’ll flesh out by the end of this tutorial. It has been said in other words that async IO gives a feeling of concurrency despite using a single thread in a single process. Coroutines (a central feature of async IO) can be scheduled concurrently, but they are not inherently concurrent.</p>

<p>That leaves one more term. What does it mean for something to be asynchronous? This isn’t a rigorous definition, but for our purposes here, I can think of two properties:
    Asynchronous routines are able to “pause” while waiting on their ultimate result and let other routines run in the meantime.</p>

<p>At the heart of async IO are coroutines. A coroutine is a specialized version of a Python generator function. Let’s start with a baseline definition and then build off of it as you progress here: a coroutine is a function that can suspend its execution before reaching return, and it can indirectly pass control to another coroutine for some time.</p>

<p>The keyword await passes function control back to the event loop. (It suspends the execution of the surrounding coroutine.) If Python encounters an await f() expression in the scope of g(), this is how await tells the event loop, “Suspend execution of g() until whatever I’m waiting on—the result of f()—is returned. In the meantime, go let something else run.”</p>

<p>For now, just know that an awaitable object is either (1) another coroutine or (2) an object defining an .<strong>await</strong>() dunder method that returns an iterator. If you’re writing a program, for the large majority of purposes, you should only need to worry about case #1.</p>

<p>That brings us to one more technical distinction that you may see pop up: an older way of marking a function as a coroutine is to decorate a normal def function with @asyncio.coroutine. The result is a generator-based coroutine. This construction has been outdated since the async/await syntax was put in place in Python 3.5.</p>

<p>While “making random integers” (which is CPU-bound more than anything) is maybe not the greatest choice as a candidate for asyncio, it’s the presence of asyncio.sleep() in the example that is designed to mimic an IO-bound process where there is uncertain wait time involved.</p>

<p>This isn’t very interesting on its surface. The result of calling a coroutine on its own is an awaitable coroutine object.</p>

<p>Time for a quiz: what other feature of Python looks like this? (What feature of Python doesn’t actually “do much” when it’s called on its own?)</p>

<p>Hopefully you’re thinking of generators as an answer to this question, because coroutines are enhanced generators under the hood. The behavior is similar in this regard:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; def gen():
...     yield 0x10, 0x20, 0x30
...
&gt;&gt;&gt; g = gen()
&gt;&gt;&gt; g  # Nothing much happens - need to iterate with `.__next__()`
&lt;generator object gen at 0x1012705e8&gt;
&gt;&gt;&gt; next(g)
(16, 32, 48)
</code></pre></div></div>

<p>Technically, await is more closely analogous to yield from than it is to yield. (But remember that yield from x() is just syntactic sugar to replace for i in x(): yield i.)</p>

<p>One critical feature of generators as it pertains to async IO is that they can effectively be stopped and restarted at will. For example, you can break out of iterating over a generator object and then resume iteration on the remaining values later. When a generator function reaches yield, it yields that value, but then it sits idle until it is told to yield its subsequent value.</p>

<p>This is the fundamental difference between functions and generators. A function is all-or-nothing. Once it starts, it won’t stop until it hits a return, then pushes that value to the caller (the function that calls it). A generator, on the other hand, pauses each time it hits a yield and goes no further. Not only can it push this value to calling stack, but it can keep a hold of its local variables when you resume it by calling next() on it.</p>

<p>There’s a second and lesser-known feature of generators that also matters. You can send a value into a generator as well through its .send() method. This allows generators (and coroutines) to call (await) each other without blocking. I won’t get any further into the nuts and bolts of this feature, because it matters mainly for the implementation of coroutines behind the scenes, but you shouldn’t ever really need to use it directly yourself.</p>

<p>Let’s try to condense all of the above articles into a few sentences: there is a particularly unconventional mechanism by which these coroutines actually get run. Their result is an attribute of the exception object that gets thrown when their .send() method is called. There’s some more wonky detail to all of this, but it probably won’t help you use this part of the language in practice, so let’s move on for now.</p>

<h2 id="side-note-low-level-implementation-of-generators">Side note. Low level implementation of generators</h2>
<p>https://leanpub.com/insidethepythonvirtualmachine/read#leanpub-auto-generators-behind-the-scenes</p>

<p>contains the yield statement, so calling it will not return a simple value as a conventional function would do. Instead, it will return a generator object which captures the continuation of the computation
We can then use the next function to get successive values from the returned generator object or send values into the generator using the send method of the generator object.</p>

<p>Cpython. We can see that a generator object contains a frame object and a code object, two objects that are essential to the execution of Python bytecode.</p>

<p>The following comprise the main attributes of a generator object.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prefix##_frame: This field references a frame object. This frame object contains the code object of a generator and it is within this frame that the execution of the generator object’s code object takes place.
prefix##_running: This is a boolean field that indicates whether the generator is running.
prefix##_code: This field references the code object associated with the generator. This is the code object that executes whenever the generator is running.
</code></pre></div></div>

<p>During the execution of the code object for the function, recall the _PyEval_EvalCodeWithName is invoked to perform some setup. During this setup process, the interpreter checks if the CO_GENERATOR flag; if set, it creates and returns a generator object rather than call the evaluation loop function. The magic happens at the last code block of the _PyEval_EvalCodeWithName as shown in listing 12.2.</p>

<p>We can see from Listing 12.2 that bytecode for a generator function code object is never executed at the point of the function call - the execution of bytecode only happens when the returned generator object is running, and we look at this next.</p>

<p>We can run a generator object by passing it as an argument to the next builtin function. This will cause the generator to execute until it hits a yield expression then it suspends execution. The critical question here is how the generators can capture the execution state and update those at will.</p>

<p>Now that we know how a generator object captures execution state, we move to the question of how the execution of a suspended generator object is resumed, and this is not too hard to figure out given the information that we have already. When the next builtin function is called with a generator as an argument, the next function dereferences the tp_iternext field of the generator type and invokes whatever function that field references. In the case of a generator object, that field references a function, gen_iternext, which calls the gen_send_ex function, that does the actual work of resuming the execution of the generator object. Before the generator object was created, the initial setup of the frame object and variables was carried out by the _PyEval_EvalCodeWithName function, so the execution of the generator object involves calling the PyEval_EvalFrameEx with the frame object contained within the generator object as the frame argument. The execution of the code object contained within the frame then proceeds as explained the chapter on the evaluation loop.</p>

<p>Python generators do more than just generate values; they can also consume values by using the generator send method. This is possible because yield is an expression that evaluates to a value. When the send method is called on a generator with a value, the gen_send_ex method places the value onto the evaluation stack of the generator object frame before the evaluation of the frame object resumes. Listing 12.3 shows the STORE_FAST instruction comes after YIELD_VALUE; this stores the value at the top of the stack to the provided name. In the case where there is no send function call, then the None value is placed on the top of the stack.</p>

<h2 id="more-about-the-roots-of-asyncawait-in-generators">More about the roots of async/await in generators</h2>
<p>https://snarky.ca/how-the-heck-does-async-await-work-in-python-3-5/</p>

<p>But until recently I didn’t understand how async/await worked in Python 3.5. I knew that yield from in Python 3.3 combined with asyncio in Python 3.4 had led to this new syntax. But having not done a lot of networking stuff – which asyncio is not limited to but does focus on – had led to me not really paying much attention to all of this async/await stuff.</p>

<p>Having a function pause what it is doing whenever it hit a yield expression – although it was a statement until Python 2.5 – and then be able to resume later is very useful in terms of using less memory, allowing for the idea of infinite sequences, etc.</p>

<p>But as you may have noticed, generators are all about iterators. Now having a better way to create iterators is obviously great (and this is shown when you define an <strong>iter</strong>() method on an object as a generator), but people knew that if we took the “pausing” part of generators and added in a “send stuff back in” aspect to them, Python would suddenly have the concept of coroutines in Python</p>

<p>Among other things, PEP 342 introduced the send() method on generators. This allowed one to not only pause generators, but to send a value back into a generator where it paused.</p>

<p>Generators were not mucked with again until Python 3.3 when PEP 380 added yield from. Strictly speaking, the feature empowers you to refactor generators in a clean way by making it easy to yield every value from an iterator (which a generator conveniently happens to be).</p>

<p>By virtue of making refactoring easier, yield from also lets you chain generators together so that values bubble up and down the call stack without code having to do anything special.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def bottom():
    # Returning the yield lets the value that goes up the call stack to come right back
    # down.
    return (yield 42)

def middle():
    return (yield from bottom())

def top():
    return (yield from middle())

# Get the generator.
gen = top()
value = next(gen)
print(value)  # Prints '42'.
try:
    value = gen.send(value * 2)
except StopIteration as exc:
    value = exc.value
print(value)  # Prints '84'
</code></pre></div></div>

<p>It’s important to understand what an event loop is and how they make asynchronous programming possible if you’re going to care about async/await. If you have done GUI programming before – including web front-end work – then you have worked with an event loop.</p>

<p>Going back to Wikipedia, an event loop “is a programming construct that waits for and dispatches events or messages in a program”. Basically an event loop lets you go, “when A happens, do B”. Probably the easiest example to explain this is that of the JavaScript event loop that’s in every browser. Whenever you click something (“when A happens”), the click is given to the JavaScript event loop which checks if any onclick callback was registered to handle that click (“do B”). If any callbacks were registered then the callback is called with the details of the click. The event loop is considered a loop because it is constantly collecting events and loops over them to find what to do with the event.</p>

<p>In Python’s case, asyncio was added to the standard library to provide an event loop. There’s a focus on networking in asyncio which in the case of the event loop is to make the “when A happens” to be when I/O from a socket is ready for reading and/or writing (via the selectors module). Other than GUIs and I/O, event loops are also often used for executing code in another thread or subprocess and have the event loop act as the scheduler (i.e., cooperative multitasking). If you happen to understand Python’s GIL, event loops are useful in cases where releasing the GIL is possible and useful.</p>

<p>Asynchronous programming is basically programming where execution order is not known ahead of time (hence asynchronous instead of synchronous).</p>]]></content><author><name>TomDev</name><email>tommog2499@gmail.com</email></author><category term="programming" /><summary type="html"><![CDATA[In computer programming, a runtime library is a set of low-level routines used by a compiler to invoke some of the behaviors of a runtime environment, by inserting calls to the runtime library into compiled executable binary. The runtime environment implements the execution model, built-in functions, and other fundamental behaviors of a programming language.[1] During execution (run time) of that computer program, execution of those calls to the runtime library cause communication between the executable binary and the runtime environment. A runtime library often includes built-in functions for memory management or exception handling.[2] Therefore, a runtime library is always specific to the platform and compiler.]]></summary></entry><entry xml:lang="ru"><title type="html">‘Postman не нужен, родной’ или используем cURL для API запросов</title><link href="/programming/2023/10/25/multiprocess.html" rel="alternate" type="text/html" title="‘Postman не нужен, родной’ или используем cURL для API запросов" /><published>2023-10-25T00:00:00+03:00</published><updated>2023-10-25T00:00:00+03:00</updated><id>/programming/2023/10/25/multiprocess</id><content type="html" xml:base="/programming/2023/10/25/multiprocess.html"><![CDATA[<p>I’ll explain it more in details again later, but first of all, I believe it is important to define the difference between concurrency and parallelism.</p>

<p>In many places both words refer to the same concept. They are often used as two different ideas in the context of Erlang. For many Erlangers, concurrency refers to the idea of having many actors running independently, but not necessarily all at the same time. Parallelism is having actors running exactly at the same time. I will say that there doesn’t seem to be any consensus on such definitions around various areas of computer science, but I will use them in this manner in this text. Don’t be surprised if other sources or people use the same terms to mean different things.</p>

<p>This is to say Erlang had concurrency from the beginning, even when everything was done on a single core processor in the ’80s. Each Erlang process would have its own slice of time to run, much like desktop applications did before multi-core systems.</p>

<p>Parallelism was still possible back then; all you needed to do was to have a second computer running the code and communicating with the first one. Even then, only two actors could be run in parallel in this setup. Nowadays, multi-core systems allows for parallelism on a single computer (with some industrial chips having many dozens of cores) and Erlang takes full advantage of this possibility.</p>

<p>The distinction between concurrency and parallelism is important to make, because many programmers hold the belief that Erlang was ready for multi-core computers years before it actually was. Erlang was only adapted to true symmetric multiprocessing in the mid 2000s and only got most of the implementation right with the R13B release of the language in 2009. Before that, SMP often had to be disabled to avoid performance losses. To get parallelism on a multicore computer without SMP, you’d start many instances of the VM instead.</p>

<p>An interesting fact is that because Erlang concurrency is all about isolated processes, it took no conceptual change at the language level to bring true parallelism to the language. All the changes were transparently done in the VM, away from the eyes of the programmers.</p>

<p>To make it efficient, it made sense for processes to be started very quickly, to be destroyed very quickly and to be able to switch them really fast. Having them lightweight was mandatory to achieve this. It was also mandatory because you didn’t want to have things like process pools (a fixed amount of processes you split the work between.) Instead, it would be much easier to design programs that could use as many processes as they need.</p>

<p>Anyway, to get back to small processes, because telephony applications needed a lot of reliability, it was decided that the cleanest way to do things was to forbid processes from sharing memory. Shared memory could leave things in an inconsistent state after some crashes (especially on data shared across different nodes) and had some complications. Instead, processes should communicate by sending messages where all the data is copied. This would risk being slower, but safer.</p>

<p>Alright, so it was decided that lightweight processes with asynchronous message passing were the approach to take for Erlang. How to make this work? Well, first of all, the operating system can’t be trusted to handle the processes. Operating systems have many different ways to handle processes, and their performance varies a lot. Most if not all of them are too slow or too heavy for what is needed by standard Erlang applications. By doing this in the VM, the Erlang implementers keep control of optimization and reliability. Nowadays, Erlang’s processes take about 300 words of memory each and can be created in a matter of microseconds—not something doable on major operating systems these days.</p>

<p>To handle all these potential processes your programs could create, the VM starts one thread per core which acts as a scheduler. Each of these schedulers has a run queue, or a list of Erlang processes on which to spend a slice of time. When one of the schedulers has too many tasks in its run queue, some are migrated to another one. This is to say each Erlang VM takes care of doing all the load-balancing and the programmer doesn’t need to worry about it. There are some other optimizations that are done, such as limiting the rate at which messages can be sent on overloaded processes in order to regulate and distribute the load.</p>

<p>Your parallel program only goes as fast as its slowest sequential part.</p>

<p>The difficulty of obtaining linear scaling is not due to the language itself, but rather to the nature of the problems to solve. Problems that scale very well are often said to be embarrassingly parallel. (raytracing)</p>

<p>https://blog.stenmans.org/theBeamBook/#CH-Scheduling</p>

<p>##</p>

<p>A CPU core is a CPU’s processor. In the old days, every processor had just one core that could focus on one task at a time. Today, CPUs have been two and 18 cores, each of which can work on a different task. As you can see in our CPU Benchmarks Hierarchy, that can have a huge impact on performance.</p>

<p>A core can work on one task, while another core works a different task, so the more cores a CPU has, the more efficient it is. Many processors, especially those in laptops, have two cores, but some laptop CPUs (known as mobile CPUs), such as Intel’s 8th Generation processors, have four. You should shoot for at least four cores in your machine if you can afford it.</p>

<p>Most processors can use a process called simultaneous multithreading or, if it’s an Intel processor, Hyper-threading (the two terms mean the same thing) to split a core into virtual cores, which are called threads. For example, AMD CPUs with four cores use simultaneous multithreading to provide eight threads, and most Intel CPUs with two cores use Hyper-threading to provide four threads.</p>

<p>Some apps take better advantage of multiple threads than others. Lightly-threaded apps, like games, don’t benefit from a lot of cores, while most video editing and animation programs can run much faster with extra threads.</p>

<p>##</p>

<p>https://stackoverflow.com/questions/980999/what-does-multicore-assembly-language-look-like
first answer
https://www.codeproject.com/Articles/889245/Deep-Inside-CPU-Raw-Multicore-Programming
https://www.reddit.com/r/learnprogramming/comments/va0tpm/how_is_multithreading_implemented_at_assembly/</p>

<p>Advanced Programmable Interrupt Controller APIC
https://en.wikipedia.org/wiki/Advanced_Programmable_Interrupt_Controller</p>

<h2 id="linux-process">linux process</h2>

<p>In many operating systems, a common design paradigm is to separate
high-level policies from their low-level mechanisms [L+75]. You can
think of the mechanism as providing the answer to a how question about
a system; for example, how does an operating system perform a context
switch? The policy provides the answer to a which question; for example,
which process should the operating system run right now? Separating the
two allows one easily to change policies without having to rethink the
mechanism and is thus a form of modularity, a general software design
principle.
gram is currently being executed; similarly a stack pointer and associated</p>

<p>In order to virtualize the CPU, the operating system needs to somehow
share the physical CPU among many jobs running seemingly at the same
time. The basic idea is simple: run one process for a little while, then
run another one, and so forth. By time sharing the CPU in this manner,
virtualization is achieved.
There are a few challenges, however, in building such virtualization</p>

<p>virtualization is achieved.
There are a few challenges, however, in building such virtualization
machinery. The first is performance: how can we implement virtualiza-
tion without adding excessive overhead to the system? The second is
control: how can we run processes efficiently while retaining control over
the CPU? Control is particularly important to the OS, as it is in charge of
resources; without control, a process could simply run forever and take
over the machine, or access information that it should not be allowed to
access. Obtaining high performance while maintaining control is thus
one of the central challenges in building an operating system.</p>

<p>One approach would simply be to let any process do whatever it wants
in terms of I/O and other related operations. However, doing so would
prevent the construction of many kinds of systems that are desirable. For
example, if we wish to build a file system that checks permissions before
granting access to a file, we can’t simply let any user process issue I/Os
to the disk; if we did, a process could simply read or write the entire disk
and thus all protections would be lost.
Thus, the approach we take is to introduce a new processor mode,
known as user mode; code that runs in user mode is restricted in what it
can do. For example, when running in user mode, a process can’t issue
I/O requests; doing so would result in the processor raising an exception;
the OS would then likely kill the process.
In contrast to user mode is kernel mode, which the operating system</p>

<p>The hardware assists the OS by providing different modes of execution.
In user mode, applications do not have full access to hardware resources.
In kernel mode, the OS has access to the full resources of the machine.
Special instructions to trap into the kernel and return-from-trap back to
user-mode programs are also provided, as well instructions that allow the
OS to tell the hardware where the trap table resides in memory.</p>

<p>concise subset of around twenty calls.
To execute a system call, a program must execute a special trap instruc-
tion. This instruction simultaneously jumps into the kernel and raises the
privilege level to kernel mode; once in the kernel, the system can now per-
form whatever privileged operations are needed (if allowed), and thus do
the required work for the calling process. When finished, the OS calls a
special return-from-trap instruction, which, as you might expect, returns
into the calling user program while simultaneously reducing the privi-
lege level back to user mode.
The hardware needs to be a bit careful when executing a trap, in that it</p>

<p>to return correctly when the OS issues the return-from-trap instruction.
On x86, for example, the processor will push the program counter, flags,
and a few other registers onto a per-process kernel stack; the return-from-
trap will pop these values off the stack and resume execution of the user-
mode program (see the Intel systems manuals [I11] for details). Other
hardware systems use different conventions, but the basic concepts are
similar across platforms.
There is one important detail left out of this discussion: how does the</p>

<p>Problem #2: Switching Between Processes
The next problem with direct execution is achieving a switch between
processes. Switching between processes should be simple, right? The
OS should just decide to stop one process and start another. What’s the
big deal? But it actually is a little bit tricky: specifically, if a process is
running on the CPU, this by definition means the OS is not running. If
the OS is not running, how can it do anything at all? (hint: it can’t) While
this sounds almost philosophical, it is a real problem: there is clearly no
way for the OS to take an action if it is not running on the CPU. Thus we
arrive at the crux of the problem.</p>

<p>the offending process).
Thus, in a cooperative scheduling system, the OS regains control of
the CPU by waiting for a system call or an illegal operation of some kind
to take place. You might also be thinking: isn’t this passive approach less
than ideal? What happens, for example, if a process (whether malicious,
or just full of bugs) ends up in an infinite loop, and never makes a system
call? What can the OS do then
?</p>

<p>over the machine?
The answer turns out to be simple and was discovered by a number
of people building computer systems many years ago: a timer interrupt
[M+63]. A timer device can be programmed to raise an interrupt every
so many milliseconds; when the interrupt is raised, the currently running
process is halted, and a pre-configured interrupt handler in the OS runs.
At this point, the OS has regained control of the CPU, and thus can do
what it pleases: stop the current process, and start a different one.</p>

<p>Saving and Restoring Context
Now that the OS has regained control, whether cooperatively via a sys-
tem call, or more forcefully via a timer interrupt, a decision has to be
made: whether to continue running the currently-running process, or
switch to a different one. This decision is made by a part of the operating
system known as the scheduler; we will discuss scheduling policies in
great detail in the next few chapters.
If the decision is made to switch, the OS then executes a low-level
piece of code which we refer to as a context switch. A context switch is
conceptually simple: all the OS has to do is save a few register values
for the currently-executing process (onto its kernel stack, for example)
and restore a few for the soon-to-be-executing process (from its kernel
stack). By doing so, the OS thus ensures that when the return-from-trap
instruction is finally executed, instead of returning to the process that was
running, the system resumes execution of another process.
To save the context of the currently-running process, the OS will exe-</p>

<p>To whet your appetite, we’ll just sketch some basics of how the OS
handles these tricky situations. One simple thing an OS might do is dis-
able interrupts during interrupt processing; doing so ensures that when
one interrupt is being handled, no other one will be delivered to the CPU.
Of course, the OS has to be careful in doing so; disabling interrupts for
too long could lead to lost interrupts, which is (in technical terms) bad.
Operating systems also have developed a number of sophisticated
locking schemes to protect concurrent access to internal data structures.
This enables multiple activities to be on-going within the kernel at the
same time, particularly useful on multiprocessors. As we’ll see in the
next piece of this book on concurrency, though, such locking can be com-
plicated and lead to a variety of interesting and hard-to-find bugs.</p>

<p>Single-Queue Scheduling
With this background in place, we now discuss how to build a sched-
uler for a multiprocessor system. The most basic approach is to simply
reuse the basic framework for single processor scheduling, by putting all
jobs that need to be scheduled into a single queue; we call this single-
queue multiprocessor scheduling or SQMS for short. This approach
has the advantage of simplicity; it does not require much work to take an
existing policy that picks the best job to run next and adapt it to work on
more than one CPU (where it might pick the best two jobs to run, if there
are two CPUs, for example).</p>

<p>of scalability. To ensure the scheduler works correctly on multiple CPUs,
the developers will have inserted some form of locking into the code, as
described above. Locks ensure that when SQMS code accesses the single
queue (say, to find the next job to run), the proper outcome arises.
Locks, unfortunately, can greatly reduce performance, particularly as</p>

<p>0.5 Multi-Queue Scheduling
Because of the problems caused in single-queue schedulers, some sys-
tems opt for multiple queues, e.g., one per CPU. We call this approach
multi-queue multiprocessor scheduling (or MQMS).
In MQMS, our basic scheduling framework consists of multiple schedul-
ing queues. Each queue will likely follow a particular scheduling disci-
pline, such as round robin, though of course any algorithm can be used.
When a job enters the system, it is placed on exactly one scheduling
queue, according to some heuristic (e.g., random, or picking one with
fewer jobs than others). Then it is scheduled essentially independently,
thus avoiding the problems of information sharing and synchronization
found in the single-queue approach.
For example, assume we have a system where there are just two CPUs</p>

<p>contents therein.
But, if you’ve been paying attention, you might see that we have a new
problem, which is fundamental in the multi-queue based approach: load
imbalance. Let’s assume we have the same set up as above (four jobs,
two CPUs), but then one of the jobs (say C) finishes. We now have the
following scheduling queues:</p>

<p>The obvious answer to this query is to move jobs around, a technique
which we (once again) refer to as migration. By migrating a job from one
CPU to another, true load balance can be achieved.
Let’s look at a couple of examples to add some clarity. Once again, we</p>

<p>В ассембле все равно будет вызов
<code class="language-plaintext highlighter-rouge">call    pthread_create@PLT</code></p>

<h2 id="go-language">Go language</h2>

<p>Each operating system thread has a fixed-size block memory (sometimes as large as 2MB) for its stack, which is the work area where it saves the local variables of function calls that are in process or momentarily halted while another function is performed. This fixed-size stack is both too big and too small. A 2MB stack would be a tremendous waste of memory for a small goroutine that simply waits for a WaitGroup before closing a channel.</p>

<p>It is not uncommon for a Go program to generate hundreds of thousands of goroutines at once, which would be difficult to stack. Regardless of size, fixed-size stacks are not always large enough for the most complex and deeply recursive routines. Changing the fixed size can improve space efficiency and allow for the creation of more threads, or it can permit more deeply recursive algorithms, but not both.</p>

<p>A goroutine, on the other hand, starts with a modest stack, typically 2KB. The stack of a goroutine, like the stack of an OS thread, maintains the local variable of active and suspended function calls, but unlike the stack of an OS thread, the stack of a goroutine is not fixed; it grows and shrinks as needed. A goroutine stack’s size limit could be as much as 1GB, which is orders of magnitude larger than a conventional fixed-size thread stack; however, few goroutines use that much.</p>

<p>https://www.developer.com/languages/go-scheduler/
https://hadar.gr/2017/lightweight-goroutines
https://stackoverflow.com/questions/24599645/how-do-goroutines-work-or-goroutines-and-os-threads-relation</p>

<p>wiki go</p>

<p>https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/</p>

<p>hree more languages that don’t have this problem: Go, Lua, and Ruby.</p>

<p>Any guess what they have in common?</p>

<p>Threads. Or, more precisely: multiple independent callstacks that can be switched between. It isn’t strictly necessary for them to be operating system threads. Goroutines in Go, coroutines in Lua, and fibers in Ruby are perfectly adequate.</p>

<p>The fundamental problem is “How do you pick up where you left off when an operation completes”? You’ve built up some big callstack and then you call some IO operation. For performance, that operation uses the operating system’s underlying asynchronous API. You cannot wait for it to complete because it won’t. You have to return all the way back to your language’s event loop and give the OS some time to spin before it will be done.</p>

<p>Once operation completes, you need to resume what you were doing. The usual way a language “remembers where it is” is the callstack. That tracks all of the functions that are currently being invoked and where the instruction pointer is in each one.</p>

<p>But to do async IO, you have to unwind and discard the entire C callstack. Kind of a Catch-22. You can do super fast IO, you just can’t do anything with the result! Every language that has async IO in its core—or in the case of JS, the browser’s event loop—copes with this in some way.</p>

<p>This is why async-await didn’t need any runtime support in the .NET framework. The compiler compiles it away to a series of chained closures that it can already handle. (Interestingly, closures themselves also don’t need runtime support. They get compiled to anonymous classes. In C#, closures really are a poor man’s objects.)</p>

<h2 id="multiprocessing-module-in-python">multiprocessing module in Python</h2>

<p>multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. It runs on both POSIX and Windows.</p>

<p>The multiprocessing module also introduces APIs which do not have analogs in the threading module. A prime example of this is the Pool object which offers a convenient means of parallelizing the execution of a function across multiple input values, distributing the input data across processes (data parallelism).</p>

<h3 id="sources">Sources</h3>

<p>https://learnyousomeerlang.com/the-hitchhikers-guide-to-concurrency
https://www.tomshardware.com/news/cpu-core-definition,37658.html
os three easy pieces</p>]]></content><author><name>TomDev</name><email>tommog2499@gmail.com</email></author><category term="programming" /><summary type="html"><![CDATA[I’ll explain it more in details again later, but first of all, I believe it is important to define the difference between concurrency and parallelism.]]></summary></entry><entry xml:lang="ru"><title type="html">‘Postman не нужен, родной’ или используем cURL для API запросов</title><link href="/programming/2022/02/01/curl_as_postman_ru.html" rel="alternate" type="text/html" title="‘Postman не нужен, родной’ или используем cURL для API запросов" /><published>2022-02-01T00:00:00+03:00</published><updated>2022-02-01T00:00:00+03:00</updated><id>/programming/2022/02/01/curl_as_postman_ru</id><content type="html" xml:base="/programming/2022/02/01/curl_as_postman_ru.html"><![CDATA[<p>Давайте рассмотрим, как с помощью cURL можно делать <code class="language-plaintext highlighter-rouge">GET, POST, PUT, PATCH, and DELETE</code> http запросы.
Для примеров будем использовать <a href="https://jsonplaceholder.typicode.com/posts">jsonplaceholder</a></p>

<h3 id="флаги-c">Флаги c</h3>

<ul>
  <li>-X –request - Custom request method</li>
  <li>-d –data - Sends the specified data</li>
  <li>-H –header - Sends headers</li>
  <li>-i –include - Display response headers</li>
</ul>

<h3 id="get">GET</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>curl https://jsonplaceholder.typicode.com/posts
</code></pre></div></div>

<h3 id="post">POST</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>curl <span class="nt">-X</span> POST <span class="nt">-d</span> <span class="s2">"userId=5&amp;title=Stuff and Things&amp;body=An amazing blog post about both stuff and things."</span> https://jsonplaceholder.typicode.com/posts
</code></pre></div></div>
<p>urlencoded</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>curl <span class="nt">-X</span> POST <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="nt">-d</span> <span class="s1">'{"userId": 5, "title": "Stuff and Things", "body": "An amazing blog post about both stuff and things."}'</span>
https://jsonplaceholder.typicode.com/posts
</code></pre></div></div>
<p>json</p>

<h3 id="put">PUT</h3>

<h3 id="patch">PATCH</h3>

<h3 id="delete">DELETE</h3>

<h3 id="authentication">Authentication</h3>

<h3 id="sources">Sources</h3>

<h3 id="let-there-be-vim">Let there be vim</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">fc</span>
</code></pre></div></div>
<p>export EDITOR=nvim</p>

<p>Using Vim As Your Shell Command-Line Scratch</p>

<p>In that case, what we can do is press ctrl + a to move the cursor before the first character in your command line and type pound (#),
 this results into making the command as a comment.</p>

<p>:%norm A \</p>
<ul>
  <li>’:%norm’ perform a normal command for the whole buffer</li>
  <li>‘A’ go to insert mode to the end of the line</li>
  <li>’ ' add literal space and backslash</li>
</ul>

<p>How to solve this? You need to use :cq to exit vim with error code. You can add a key bind for this, or you can just delete the content of your vim buffer with dd and exit.</p>

<table>
  <tbody>
    <tr>
      <td>echo hello</td>
      <td>nvim -</td>
    </tr>
  </tbody>
</table>

<p>jq utility https://stedolan.github.io/jq/</p>

<p>:w !bash</p>

<p>We can also pick which line to execute.</p>

<p>:.w !zsh</p>

<p>Notice the dot(.) before w, this means “write the current line of the cursor to the shell”.</p>

<p>SHLVL</p>

<p>https://www.taniarascia.com/making-api-requests-postman-curl/</p>

<p>We can improve our workflow by adding intellisense using the Language Server Protocol (LSP).</p>

<ul>
  <li>https://github.com/curl/curl/blob/master/docs/MANUAL.md</li>
  <li>https://man7.org/linux/man-pages/man1/curl.1.html</li>
  <li>https://dev.to/zaerald/using-vim-as-your-shell-command-line-scratch-1lcl</li>
</ul>]]></content><author><name>TomDev</name><email>tommog2499@gmail.com</email></author><category term="programming" /><summary type="html"><![CDATA[Давайте рассмотрим, как с помощью cURL можно делать GET, POST, PUT, PATCH, and DELETE http запросы. Для примеров будем использовать jsonplaceholder]]></summary></entry><entry xml:lang="ru"><title type="html">Псевдо 3D с помощью Raycasting</title><link href="/programming/2021/01/20/raycast_ru.html" rel="alternate" type="text/html" title="Псевдо 3D с помощью Raycasting" /><published>2021-01-20T00:00:00+03:00</published><updated>2021-01-20T00:00:00+03:00</updated><id>/programming/2021/01/20/raycast_ru</id><content type="html" xml:base="/programming/2021/01/20/raycast_ru.html"><![CDATA[<p>Сегодня мы рассмотрим алгоритм, который (возможно) впервые был использован в игре Wolfenstein 3D 1992 года.</p>

<p>Ray casting можно перевести как “бросание лучей” и это на самом деле достаточно хорошее описание. По сути, весь
алгоритм сводится к тому, что мы бросаем N-ое количество лучей и на основание расстояния каждого луча отображаем вертикальную линию.</p>

<p>Итак, наши действующие лица:</p>
<ul>
  <li>2D плоскоcть (карта)</li>
  <li>Игрок который может передвигаться по 2D плоскоcти</li>
</ul>

<p>Для простоты, карту будем представлять в виде 2-мерного массива 10 на 10 (хотя есть варианты по оптимальнее) где 1 и 0 означают:<br />
1 - ячейка заполнена
0 - пустое место</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">world_map</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">]</span>
</code></pre></div></div>

<p>Размер “ячейки” возьмем за 80 пикселей (она у нас квадратная). Так как у нас карта 10x10 ячеек, удобно сделать так, чтобы размер экран был
800x800(то есть мы просто домножили на размер ячейки)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">screen_w</span><span class="sh">"</span> <span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">screen_h</span><span class="sh">"</span> <span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">fps</span><span class="sh">'</span> <span class="p">:</span> <span class="mi">60</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">tile</span><span class="sh">'</span> <span class="p">:</span> <span class="mi">80</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Базовые вещи, связанные с pygame я опускаю. Cоздав игровой цикл, поместим внутри него функцию, которая будет отрисовывать карту</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">draw_map</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">world_map</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">world_map</span><span class="p">[</span><span class="n">y</span><span class="p">])):</span>
            <span class="k">if</span> <span class="n">world_map</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pygame</span><span class="p">.</span><span class="n">draw</span><span class="p">.</span><span class="nf">rect</span><span class="p">(</span><span class="n">screen</span><span class="p">,</span> <span class="n">colors</span><span class="p">[</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">],</span> \
                    <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">tile</span><span class="sh">'</span><span class="p">],</span> <span class="n">y</span> <span class="o">*</span> <span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">tile</span><span class="sh">'</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">tile</span><span class="sh">'</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">tile</span><span class="sh">'</span><span class="p">]),</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>TomDev</name><email>tommog2499@gmail.com</email></author><category term="programming" /><summary type="html"><![CDATA[Сегодня мы рассмотрим алгоритм, который (возможно) впервые был использован в игре Wolfenstein 3D 1992 года.]]></summary></entry></feed>